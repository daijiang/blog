<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>R on The stupidest thing...</title>
    <link>/tags/r/</link>
    <description>Recent content in R on The stupidest thing...</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 01 May 2017 00:10:00 -0500</lastBuildDate>
    
	<atom:link href="/tags/r/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>caching blogdown</title>
      <link>/2017/05/01/caching-blogdown/</link>
      <pubDate>Mon, 01 May 2017 00:10:00 -0500</pubDate>
      
      <guid>/2017/05/01/caching-blogdown/</guid>
      <description>blogdown is cool, but it seems that it builds every .Rmd file every time you run build_site(), so if your site includes an analysis that takes a while…well, every time you build the site it’s going to take a while.
I had the mistaken impression that blogdown would look at the timestamps on the .Rmd and .html and only build the .html if the .Rmd file is newer, but that’s not true.</description>
    </item>
    
    <item>
      <title>sqlite, feather, and fst</title>
      <link>/2017/04/30/sqlite-feather-and-fst/</link>
      <pubDate>Sun, 30 Apr 2017 14:07:00 -0500</pubDate>
      
      <guid>/2017/04/30/sqlite-feather-and-fst/</guid>
      <description>I don’t think I’m unusual among statisticians in having avoided working directly with databases for much of my career. The data for my projects have been reasonably small. (In fact, basically all of the data for my 20 years of projects are on my laptop’s drive.) Flat files (such as CSV files) were sufficient.
But I’ve finally entered the modern era of biggish data. (Why do they call it big data?</description>
    </item>
    
  </channel>
</rss>